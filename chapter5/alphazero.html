<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Chapter 5.1: AlphaZero</title>
  <meta name="description" content="A beginner-friendly guide to linear disciminant analysis in machine learning." />


  <!-- Load MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Load Plotly.js for graphing -->
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script>

  <style>
    body {
      font-family: "Georgia", serif;
      line-height: 1.6;
      max-width: 800px;
      margin: auto;
      padding: 20px;
      background: #fafafa;
    }

    h1 {
      text-align: center;
    }

    h3 {
      text-align: center;
    }

    .math {
      font-size: 1.2em;
      background: #fff;
      padding: 10px;
      border-left: 4px solid #007acc;
      margin: 20px 0;
    }

    .inputs {
      margin-bottom: 20px;
      padding: 10px;
      background: #f0f0f0;
      border-radius: 6px;
    }

    label {
      margin-right: 10px;
    }

    #graph {
      width: 100%;
      height: 500px;
    }

    input {
      width: 60px;
    }

    .nav-bar {
  position: fixed;
  bottom: 0;
  left: 0;
  width: 100%;
  background-color: #ffffff00;
  color: white;
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px 0px;
  z-index: 1000;
  /* Removed box-shadow (edge gradient) */
}

.nav-bar button {
  background-color: rgba(255, 255, 255, 0);
  color: #007acc;
  border: none;
  padding: 8px 16px;
  font-size: 16px;
  font-weight: bold;
  cursor: pointer;
  border-radius: 4px;
  transition: background-color 0.2s ease;
  margin-right: 10px;
}

.nav-bar button:last-child {
  margin-right: 0;
}

.nav-bar button:hover {
  background-color: #f0f0f0;
}

.nav-left {
  display: flex;
  align-items: center;
}

code {
  border-left: 4px solid #11cc00; /* Blue line */
  padding-left: 12px;             /* Space between border and code */
  display: block;                 /* Ensures it spans full width */
  background-color: #f9f9f9;      /* Optional: code background */
}
  </style>

  <!-- Highlight.js stylesheet (theme: GitHub) -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">

<!-- Highlight.js library -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Initialize highlighting -->
<script>hljs.highlightAll();</script>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://yourdomain.com/lda-tutorial.html"
  },
  "headline": "Chapter 2.3: Linear Discriminant Analysis",
  "description": "A beginner-friendly guide to Linear Discriminant Analysis (LDA) in machine learning, covering both classification and dimensionality reduction with math and code examples.",
  "author": {
    "@type": "Person",
    "name": "Dominic Scocchera"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Your Website Name",
    "logo": {
      "@type": "ImageObject",
      "url": "https://yourdomain.com/logo.png"
    }
  },
  "datePublished": "2025-08-23",
  "dateModified": "2025-08-23",
  "image": "https://yourdomain.com/LDA.png",
  "keywords": [
    "Linear Discriminant Analysis", "Machine Learning", "LDA Tutorial", "Dimensionality Reduction", "Classification"
  ],
  "codeSampleType": "full",
  "programmingLanguage": "Python",
  "articleSection": "Machine Learning",
  "isPartOf": {
    "@type": "CreativeWorkSeries",
    "name": "Machine Learning Tutorials"
  }
}
</script>
</head>
<body>

  <h1>Chapter 5.1: AlphaZero</h1>

  <p>One of the most impressive models in machine learning in recent years was Google Deepminds <a href="https://www.youtube.com/watch?v=WXuK6gekU1Y">AlphaGo model</a> which was able to beat
     the worlds top Go player. In this tutorial we will go over the basics of AlphaZero (similar to AlphaGo but trains against itself instead of past matches) and implement it for a much simpler game connect four. We will start by going over the neural network backbone of
     the algorithm. We use a typical CNN architecture with input size of [batch_size, 2 (positions of player 1 pieces and positions of player 2's pieces), # of rows (6 for connect four), # of columns (7 for connect four)]. The output of this
     CNN is a probability distribution over all the possible moves to play next known as the policy head and here we will use log_softmax to convert the outputs of the network to a distribution.
     It also outputs a scalar (between -1 and 1) known as the value head that estimates the expected outcome of the current state (1 = certain win, 0 = draw, -1 = certain loss) and to constrain
     the output of the network to that range the tanh function is used.</p>
  <p>One final important thing to consider is illegal moves, for example if a column in connect four has reached the top you can't play
     another piece above it. To solve this we can first create a mask vector (M) over the columns (\(M_{i}\) is 1 if column i is legal and 0 if column i illegal). We can apply our mask to the output of the network by
     applying the mask to the logits (We don't use softmax in the network to get probabilities, it is applied after) \(z_{i}'=z_{i}\) if \(M_{i}=1\) and \(z_{i}'=-10^{9}\) if \(M_{i}=0\)
     (or any other large negative value). This ensures that after softmax is applied any illegal moves will have essentially a 0 probability of being chosen. We then renormalise and take the logarithm.
      The idea behind this is that during training the network will naturally learn to not choose illegal moves. We can see all this implemented in the code below (note we will apply the masking after the output of the network and not inside of it).
  </p>

  <pre><code class="language-python">
import torch
import torch.nn as nn
import torch.nn.functional as F

def get_legal_moves(board):  # board shape: (6, 7)
    return [1 if board[0][col] == 0 else 0 for col in range(7)]

def apply_masked_softmax(logits, legal_moves_mask):
    """
    logits: [batch_size, 7]
    legal_moves_mask: [7] — binary mask (1 for legal, 0 for illegal)
    """
    mask = legal_moves_mask.bool().unsqueeze(0)  # shape [1, 7]
    masked_logits = logits.masked_fill(~mask, float('-1e9'))
    probs = F.softmax(masked_logits, dim=1)  # shape [batch, 7]
    return probs

class AlphaZeroNet(nn.Module):
    def __init__(self, board_height=6, board_width=7, action_size=7):
        super().__init__()
        self.conv1 = nn.Conv2d(2, 64, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)

        # Policy head
        self.policy_conv = nn.Conv2d(64, 2, kernel_size=1)
        self.policy_fc = nn.Linear(2 * board_height * board_width, action_size)

        # Value head
        self.value_conv = nn.Conv2d(64, 1, kernel_size=1)
        self.value_fc1 = nn.Linear(board_height * board_width, 64)
        self.value_fc2 = nn.Linear(64, 1)

    def forward(self, x):
        # Shared CNN layers
        x = F.relu(self.conv1(x))  # input shape: [batch, 2, 6, 7]
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))

        # Policy head — outputs raw logits 
        p = F.relu(self.policy_conv(x))     # shape: [batch, 2, 6, 7]
        p = p.view(p.size(0), -1)           # flatten to [batch, 2 * 6 * 7]
        p = self.policy_fc(p)               # shape: [batch, action_size]

        # Value head
        v = F.relu(self.value_conv(x))
        v = v.view(v.size(0), -1)
        v = F.relu(self.value_fc1(v))
        v = torch.tanh(self.value_fc2(v))   # outputs in range [-1, 1]

        return p, v.squeeze(1)              # p = raw logits
  </code></pre>

  <p>The loss function we use use for the network in training is:</p>

  <div class="math">\(\mathcal{L}=\mathcal{L}_{\text{value}}+\mathcal{L}_{\text{policy}}=(v-z)^{2}-\sum_{i}\pi_{i}\log p_{a}\)</div>
  
  <p>Where v is the predicted value from the network (between -1 and 1), z is the actual game outcome (-1, 0 or 1), \(\pi\) is the target policy from Monte Carlo Tree search (we'll cover this soon) and p is the
    predicted policy from the network (after softmax over the logits). Below is the loss function in code.
  </p>

  <pre><code class="language-python">
# Loss function (remember to take log_probs as output isn't softmaxed)
def alpha_zero_loss(policy_logits, pred_value, target_policy, target_value):
    log_probs = F.log_softmax(policy_logits, dim=1)
    policy_loss = -torch.sum(target_policy * log_probs, dim=1).mean()
    value_loss = F.mse_loss(pred_value, target_value)
    return policy_loss + value_loss
  </code></pre>

  <p>The other main part of this algorithm involves Monte Carlo Tree Search.</p>

  <div class="math"></div>





 





  

  <pre><code class="language-python">
  def lda_discriminant_classifier(X_train, y_train):
    """
    Trains an LDA classifier using discriminant functions.

    Parameters:
        X_train (ndarray): Training data of shape (n_samples, n_features)
        y_train (ndarray): Class labels of shape (n_samples,)
    """
    classes = np.unique(y_train)
    n_features = X_train.shape[1]

    # Compute shared covariance matrix
    cov = np.cov(X_train.T, bias=False)
    cov_inv = np.linalg.inv(cov)

    # Compute means and priors for each class
    means = []
    priors = []
    for c in classes:
        X_c = X_train[y_train == c]
        means.append(np.mean(X_c, axis=0))
        priors.append(len(X_c) / len(X_train))
    means = np.array(means)
    priors = np.array(priors)

    return cov_inv, means, priors


# Define prediction function using discriminant function
def predict(X, cov_inv, means, priors):
    preds = []
    for x in X:
        scores = []
        for i, c in enumerate(classes):
            mu_k = means[i]
            pi_k = priors[i]
            delta_k = x @ cov_inv @ mu_k - 0.5 * mu_k @ cov_inv @ mu_k + np.log(pi_k)
            scores.append(delta_k)
        preds.append(classes[np.argmax(scores)])
    return np.array(preds)
  </code></pre>

  <p>Running this on the iris dataset we get a prediction accuracy of 84.44%. We now write the function for dimensionality reduction:</p>

  <pre><code class="language-python">
  def lda_projection_matrix(X, y, num_components=None):
    """
    Solves the generalized eigenvalue problem for LDA and returns the projection matrix W.
    
    Parameters:
        X (ndarray): Feature matrix of shape (n_samples, n_features)
        y (ndarray): Class labels of shape (n_samples,)
        num_components (int or None): Number of linear discriminants to return (<= C - 1)
        
    Returns:
        W (ndarray): Projection matrix of shape (n_features, num_components)
    """
    classes = np.unique(y)
    n_features = X.shape[1]
    n_classes = len(classes)

    # Compute overall mean
    mean_total = np.mean(X, axis=0)

    # Initialize scatter matrices
    Sw = np.zeros((n_features, n_features))  # Within-class scatter
    Sb = np.zeros((n_features, n_features))  # Between-class scatter

    for c in classes:
        X_c = X[y == c]
        mean_c = np.mean(X_c, axis=0)
        n_c = X_c.shape[0]

        # Within-class scatter
        Sw += (X_c - mean_c).T @ (X_c - mean_c)

        # Between-class scatter
        mean_diff = (mean_c - mean_total).reshape(-1, 1)
        Sb += n_c * (mean_diff @ mean_diff.T)

    # Solve generalized eigenvalue problem: Sb w = λ Sw w
    eigvals, eigvecs = np.linalg.eig(np.linalg.pinv(Sw) @ Sb)

    # Sort eigenvectors by eigenvalue magnitude (descending)
    sorted_indices = np.argsort(np.real(eigvals))[::-1]
    eigvecs = np.real(eigvecs[:, sorted_indices])

    # Select top components
    if num_components is None:
        num_components = n_classes - 1
    W = eigvecs[:, :num_components]

    return W
  </code></pre>

  <a href="https://colab.research.google.com/drive/1a6WrqZDvXukSdl6uPUHDUSsPM3WPx53k?usp=sharing" target="_blank">
  <button>Open in Google Colab</button>
  </a>

<p>We use this projection function on the iris data set and we get the following plot:</p>

<div style="text-align: center; margin-top: 40px;">
    <img src="LDA.png" alt="Plot of iris data after projection" style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
    <div style="font-size: 14px; color: #555; margin-top: 8px;">Figure: Plot of iris data after projection</div>
</div>

<p>Here we can indeed see its been reduced to \(K-1=3-1=2\) dimensions and in such a way that the classes are seperated. As an excercise use this projected data in the prediction model and see if you
    get an improved accuracy compared to what we got above (84.44%).
</p>

<!-- Bottom Navigation Bar -->
<footer style="margin-top: 80px; padding: 30px 20px; text-align: center; font-size: 14px; color: #555;">
  <div style="margin-bottom: 10px;">
    <a href="../index.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Home</a> |
    <a href="#" style="margin: 0 10px; color: #007acc; text-decoration: none;">Previous</a> |
    <a href="#" style="margin: 0 10px; color: #007acc; text-decoration: none;">Next</a> |
    <a href="../tutorialhub.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Tutorial Hub</a>
  </div>
  <div>© 2025 Dominic Scocchera. All rights reserved.</div>
</footer>

</body>
</html>
