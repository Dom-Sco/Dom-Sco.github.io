<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Chapter 2.1: Logistic Regression</title>
  <meta name="description" content="A beginner-friendly guide to logistic regression in machine learning." />

  <!-- Load MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Load Plotly.js for graphing -->
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script>

  <style>
    body {
      font-family: "Georgia", serif;
      line-height: 1.6;
      max-width: 800px;
      margin: auto;
      padding: 20px;
      background: #fafafa;
    }

    h1 {
      text-align: center;
    }

    h3 {
      text-align: center;
    }

    .math {
      font-size: 1.2em;
      background: #fff;
      padding: 10px;
      border-left: 4px solid #007acc;
      margin: 20px 0;
    }

    .inputs {
      margin-bottom: 20px;
      padding: 10px;
      background: #f0f0f0;
      border-radius: 6px;
    }

    label {
      margin-right: 10px;
    }

    #graph {
      width: 100%;
      height: 500px;
    }

    input {
      width: 60px;
    }

    .nav-bar {
  position: fixed;
  bottom: 0;
  left: 0;
  width: 100%;
  background-color: #ffffff00;
  color: white;
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px 0px;
  z-index: 1000;
  /* Removed box-shadow (edge gradient) */
}

.nav-bar button {
  background-color: rgba(255, 255, 255, 0);
  color: #007acc;
  border: none;
  padding: 8px 16px;
  font-size: 16px;
  font-weight: bold;
  cursor: pointer;
  border-radius: 4px;
  transition: background-color 0.2s ease;
  margin-right: 10px;
}

.nav-bar button:last-child {
  margin-right: 0;
}

.nav-bar button:hover {
  background-color: #f0f0f0;
}

.nav-left {
  display: flex;
  align-items: center;
}

code {
  border-left: 4px solid #11cc00; /* Blue line */
  padding-left: 12px;             /* Space between border and code */
  display: block;                 /* Ensures it spans full width */
  background-color: #f9f9f9;      /* Optional: code background */
}
  </style>

  <!-- Highlight.js stylesheet (theme: GitHub) -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">

<!-- Highlight.js library -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Initialize highlighting -->
<script>hljs.highlightAll();</script>

</head>
<body>

  <h1>Chapter 2.1: Logisitic Regression</h1>

  <p>We now move onto our first classification model, logisitic regression. In this model we only have two classes
    which we will label with 0 and 1. We can then model the probability that for a given piece of data
    that we see it is of class 1 as follows:
  </p>

  <div class="math">\(P(y=1|x)=\sigma(x^{T}\beta)=\frac{1}{1+e^{-x^{T}\beta}}\)</div>
  
  <p>This is the sigmoid function and we'll see a lot more of it later on when we work on
    neural networks (and also in the next tutorial when we extend logistic regression to multiple classes). This is because when we plot it we see that it acts as a "threshold" with mostly
    taking values close to 0 or 1 but in the middle it curves up from 0 to 1.
  </p>

  <div style="text-align: center; margin-top: 40px;">
    <img src="sigmoid.png" alt="Polynomial Regression Plot" style="max-width: 100%; height: auto; border: 1px solid #ccc; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1);">
    <div style="font-size: 14px; color: #555; margin-top: 8px;">Figure: Plot of the polynomial regression prediction vs. data</div>
  </div>

  <p>For this problem we will use a binary cross entropy loss function (BCE). We will use this as we are modelling probability
    distributions (\(P(y=1|x)\) and \(P(y=0|x)=1-P(y=1|x)\)) and BCE measures the average number of bits (0, 1) needed to identify
    an event drawn from the estimated distribution (\(P(y=1|x)\)) rather than the true distribution, whatever that may be. This is related to
    entropy from information theory but I won't go in depth about it here. BCE is defined as:
  </p>

  <div class="math">\(BCE=\mathcal{L}(\beta)=-\frac{1}{m}\sum_{i=1}^{m}[y_{i}\log(p_{i})+(1-y_{i})\log(1-p_{i})]\)</div>

  <p>In our case \(p_i=\sigma(x_{i}^{T}\beta)\). We can also rewrite our model to output all predictions as vector through matrix notation (noting that we add a 1's column for our bias) as such:</p>

  <div class="math">\(\sigma(X\beta)=\frac{1}{1+e^{-X\beta}}\)</div>

  <p>We can now perform optimisation the way we normally do through taking the gradient:</p>

  <div class="math">\(\nabla_{\beta}\mathcal{L}=\frac{1}{m}X^T(\sigma(X\beta)-y)\)</div>

  <p>I won't give a proof of it here but you can find a proof of this fact <a href="https://en.wikipedia.org/wiki/Cross-entropy#Cross-entropy_loss_function_and_logistic_regression">here</a>, or 
    you can try and prove it yourself &#128512;. If we try and set the gradient to zero and solve for \(\beta\) we will quickly realise there is no closed form solution
    and hence we can use gradient descent to find the minimum.
  </p>

  <div class="math">\(\beta^{(t+1)}=\beta^{(t)}-\eta\nabla_{\beta}\mathcal{L}(\beta^{(t)})\)</div>

  <p>Before we implement it one thing to note is that the sigmoid function can sometimes
    run into overflow errors so we only compute \(\exp(-z)\) where \(z\geq0\) and compute \(\exp(z)\)
    where \(z<0\). To ensure this we use two experessions \(\frac{1}{1+e^{-z}}\) and \(\frac{e^{z}}{1+e^{z}}\)
    which are equal to each other (multiply the first expression by \(\frac{e^{z}}{e^{z}}\)).  
  </p>

  <pre><code class="language-python">
    import numpy as np

    def sigmoid(z):
        z = np.array(z)  # ensure it's an array for element-wise ops
        result = np.empty_like(z, dtype=np.float64)

        pos_mask = z >= 0
        neg_mask = ~pos_mask

        result[pos_mask] = 1 / (1 + np.exp(-z[pos_mask]))
        result[neg_mask] = np.exp(z[neg_mask]) / (1 + np.exp(z[neg_mask]))

        return result

    def logistic_loss(X, y, beta):
        m = len(y)
        z = X @ beta
        p = sigmoid(z)
        epsilon = 1e-15  # to avoid log(0)
        return -np.mean(y * np.log(p + epsilon) + (1 - y) * np.log(1 - p + epsilon))

    def logistic_gradient(X, y, beta):
        m = len(y)
        p = sigmoid(X @ beta)
        return (1 / m) * (X.T @ (p - y))

    def logistic_regression(X, y, lr=0.1, epochs=1000):
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias term
        beta = np.zeros(X.shape[1])

        for _ in range(epochs):
            grad = logistic_gradient(X, y, beta)
            beta -= lr * grad

        return beta

    def predict(X, beta, threshold=0.5):
        X = np.c_[np.ones(X.shape[0]), X]  # Add bias
        return sigmoid(X @ beta) >= threshold
  </code></pre>

  <a href="https://colab.research.google.com/drive/1kH7I7lFaexszRktlKqPn3YYYdzJnxicD?usp=sharing" target="_blank">
  <button>Open in Google Colab</button>
  </a>

  <p>
    In the colab notebook we then train our model on the breast cancer dataset (predict whether or not a patient has breast cancer) and we
    get an accuracy of 91.56%.
</p>

  <p>
  </p>




<!-- Bottom Navigation Bar -->
<footer style="margin-top: 80px; padding: 30px 20px; text-align: center; font-size: 14px; color: #555;">
  <div style="margin-bottom: 10px;">
    <a href="../index.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Home</a> |
    <a href="classification.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Previous</a> |
    <a href="softmaxregression.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Next</a> |
    <a href="../tutorialhub.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Tutorial Hub</a>
  </div>
  <div>Â© 2025 Dominic Scocchera. All rights reserved.</div>
</footer>

</body>
</html>
