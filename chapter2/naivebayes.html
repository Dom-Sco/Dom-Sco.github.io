<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Chapter 2.5: Support Vector Machines</title>
  <meta name="description" content="A beginner-friendly guide to support vector machines in machine learning." />


  <!-- Load MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Load Plotly.js for graphing -->
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script>

  <style>
    body {
      font-family: "Georgia", serif;
      line-height: 1.6;
      max-width: 800px;
      margin: auto;
      padding: 20px;
      background: #fafafa;
    }

    h1 {
      text-align: center;
    }

    h3 {
      text-align: center;
    }

    .math {
      font-size: 1.2em;
      background: #fff;
      padding: 10px;
      border-left: 4px solid #007acc;
      margin: 20px 0;
    }

    .inputs {
      margin-bottom: 20px;
      padding: 10px;
      background: #f0f0f0;
      border-radius: 6px;
    }

    label {
      margin-right: 10px;
    }

    #graph {
      width: 100%;
      height: 500px;
    }

    input {
      width: 60px;
    }

    .nav-bar {
  position: fixed;
  bottom: 0;
  left: 0;
  width: 100%;
  background-color: #ffffff00;
  color: white;
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px 0px;
  z-index: 1000;
  /* Removed box-shadow (edge gradient) */
}

.nav-bar button {
  background-color: rgba(255, 255, 255, 0);
  color: #007acc;
  border: none;
  padding: 8px 16px;
  font-size: 16px;
  font-weight: bold;
  cursor: pointer;
  border-radius: 4px;
  transition: background-color 0.2s ease;
  margin-right: 10px;
}

.nav-bar button:last-child {
  margin-right: 0;
}

.nav-bar button:hover {
  background-color: #f0f0f0;
}

.nav-left {
  display: flex;
  align-items: center;
}

code {
  border-left: 4px solid #11cc00; /* Blue line */
  padding-left: 12px;             /* Space between border and code */
  display: block;                 /* Ensures it spans full width */
  background-color: #f9f9f9;      /* Optional: code background */
}
  </style>

  <!-- Highlight.js stylesheet (theme: GitHub) -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">

<!-- Highlight.js library -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Initialize highlighting -->
<script>hljs.highlightAll();</script>

<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://dom-sco.github.io/chapter2/naivebayes.html"
  },
  "headline": "Chapter 2.6: Naive Bayes",
  "description": "A beginner-friendly guide to Naive Bayes classification in machine learning, including Bernoulli, Multinomial, and Gaussian methods with equations and Python code.",
  "author": {
    "@type": "Person",
    "name": "Dominic Scocchera"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Dom-Sco Tutorials",
    "logo": {
      "@type": "ImageObject",
      "url": "https://dom-sco.github.io/assets/logo.png"
    }
  },
  "datePublished": "2025-08-23",
  "dateModified": "2025-08-23",
  "image": "https://dom-sco.github.io/assets/naivebayes.png",
  "keywords": [
    "Naive Bayes",
    "Machine Learning",
    "Classification",
    "Multinomial Naive Bayes",
    "Bernoulli Naive Bayes",
    "Gaussian Naive Bayes",
    "Python"
  ],
  "codeSampleType": "full",
  "programmingLanguage": "Python",
  "articleSection": "Machine Learning",
  "isPartOf": {
    "@type": "CreativeWorkSeries",
    "name": "Machine Learning Tutorials"
  }
}
</script>
</head>
<body>

  <h1>Chapter 2.6: Naive Bayes</h1>

  <p>The naive bayes classifier is a simple classifier based on bayes theorem. As this is a classification problem we use Bayes theorem as follows:</p>

  <div class="math">\(P(C|X)=\frac{P(X|C)P(C)}{P(X)}\)</div>

  <p>Where X is the data we observe and C is the class. P(C) is the prior probability of class C, i.e. the number of instances we have seen of class C divided by the number of data (same as \(\pi\) in discriminant analysis). We also make a naive assumption (hence the name of the classifier) that:</p>

  <div class="math">\(P(X_{1},X_{2},...,X_{n}|C)=P(X_{1}|C)P(X_{1}|C)...P(X_{1}|C)\)</div>

  <p>And hence the classifier becomes:</p>

  <div class="math">\(P(C|X)\propto P(C)\prod_{i=1}^{n}P(X_{i}|C)\)</div>

  <p>Note P(X) is the same for all classes so we can ignore it. We also choose the class based on which class produces the highest probability in the above expression.
    Depending on the type of classification we are doing we can calculate \(P(X_{i}|C)\) differently. Three common methods are below. They are Bernoilli, Multinomial and Gaussian respectively.</p>

  <div class="math">\(\begin{align}P(X_{i}=1|C)&=\frac{\text{number of samples in class C where }X_{i}=1}{\text{total number of samples in class C}}\\
                    P(X_{i}|C)&=\frac{\text{count}(X_{i}\text{ in class }C)}{\sum_{j=1}^{n}\text{count}(X_{j}\text{ in class }C)}\\
                    P(X_{i}|C)&=\frac{1}{\sqrt{2\pi\sigma_{C,i}^{2}}}\exp\left(-\frac{(X_{i}-\mu_{C,i})^{2}}{2\sigma_{C,i}^{2}}\right)\end{align}\)</div>

  <p>Note here that \(\mu_{C,i}\) and \(\sigma_{C,i}\) are the class means and variance respectively. Also note for Bernoilli \(P(X_{i}=0|C)=1-P(X_{i}=1|C)\). We now code up each of the classifiers below:</p>

  <pre><code class="language-python">
# ======== GAUSSIAN NAIVE BAYES =========
class GaussianNB_Scratch(NaiveBayesBase):
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.priors = {}
        self.mean = {}
        self.var = {}
        for c in self.classes:
            X_c = X[y == c]
            self.priors[c] = X_c.shape[0] / X.shape[0]
            self.mean[c] = np.mean(X_c, axis=0)
            self.var[c] = np.var(X_c, axis=0)

    def gaussian_prob(self, x, mean, var):
        eps = 1e-9
        coeff = 1.0 / np.sqrt(2.0 * np.pi * var + eps)
        exponent = -((x - mean)**2) / (2 * var + eps)
        return coeff * np.exp(exponent)

    def predict(self, X):
        preds = []
        for x in X:
            posteriors = {}
            for c in self.classes:
                prior = np.log(self.priors[c])
                likelihoods = np.log(self.gaussian_prob(x, self.mean[c], self.var[c]))
                posteriors[c] = prior + np.sum(likelihoods)
            preds.append(max(posteriors, key=posteriors.get))
        return np.array(preds)

# ======== MULTINOMIAL NAIVE BAYES =========
class MultinomialNB_Scratch(NaiveBayesBase):
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.priors = {}
        self.likelihood = {}
        self.class_totals = {}

        self.vocab_size = X.shape[1]

        for c in self.classes:
            X_c = X[y == c]
            self.priors[c] = X_c.shape[0] / X.shape[0]
            word_counts = np.sum(X_c, axis=0)
            self.likelihood[c] = (word_counts + 1) / (np.sum(word_counts) + self.vocab_size)  # Laplace smoothing

    def predict(self, X):
        preds = []
        for x in X:
            posteriors = {}
            for c in self.classes:
                prior = np.log(self.priors[c])
                likelihoods = np.sum(x * np.log(self.likelihood[c]))
                posteriors[c] = prior + likelihoods
            preds.append(max(posteriors, key=posteriors.get))
        return np.array(preds)

# ======== BERNOULLI NAIVE BAYES =========
class BernoulliNB_Scratch(NaiveBayesBase):
    def fit(self, X, y):
        self.classes = np.unique(y)
        self.priors = {}
        self.feature_probs = {}

        for c in self.classes:
            X_c = X[y == c]
            self.priors[c] = X_c.shape[0] / X.shape[0]
            feature_presence = np.sum(X_c, axis=0)
            self.feature_probs[c] = (feature_presence + 1) / (X_c.shape[0] + 2)  # Laplace smoothing

    def predict(self, X):
        preds = []
        for x in X:
            posteriors = {}
            for c in self.classes:
                prior = np.log(self.priors[c])
                likelihoods = x * np.log(self.feature_probs[c]) + (1 - x) * np.log(1 - self.feature_probs[c])
                posteriors[c] = prior + np.sum(likelihoods)
            preds.append(max(posteriors, key=posteriors.get))
        return np.array(preds)
  </code></pre>

  <a href="https://colab.research.google.com/drive/1zYFqame7o6CObMOGPqa_p9e0cowxUiGK?usp=sharing" target="_blank">
  <button>Open in Google Colab</button>
  </a>

  <p>We then train these models on Iris dataset for Gaussian, tiny text dataset for multinomial and tiny binary text for Bernoulli. We get accuracies of 97.7%, 100% and 100% respectively.</p>


<!-- Bottom Navigation Bar -->
<footer style="margin-top: 80px; padding: 30px 20px; text-align: center; font-size: 14px; color: #555;">
  <div style="margin-bottom: 10px;">
    <a href="../index.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Home</a> |
    <a href="SVM.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Previous</a> |
    <a href="#" style="margin: 0 10px; color: #007acc; text-decoration: none;">Next</a> |
    <a href="../tutorialhub.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Tutorial Hub</a>
  </div>
  <div>Â© 2025 Dominic Scocchera. All rights reserved.</div>
</footer>

</body>
</html>
