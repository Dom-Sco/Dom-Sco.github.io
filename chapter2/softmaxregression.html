<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Chapter 2.2: Softmax Regression</title>
  <meta name="description" content="A beginner-friendly guide to softmax regression in machine learning." />

  <!-- Load MathJax for LaTeX rendering -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>

  <!-- Load Plotly.js for graphing -->
  <script src="https://cdn.plot.ly/plotly-2.32.0.min.js"></script>

  <style>
    body {
      font-family: "Georgia", serif;
      line-height: 1.6;
      max-width: 800px;
      margin: auto;
      padding: 20px;
      background: #fafafa;
    }

    h1 {
      text-align: center;
    }

    h3 {
      text-align: center;
    }

    .math {
      font-size: 1.2em;
      background: #fff;
      padding: 10px;
      border-left: 4px solid #007acc;
      margin: 20px 0;
    }

    .inputs {
      margin-bottom: 20px;
      padding: 10px;
      background: #f0f0f0;
      border-radius: 6px;
    }

    label {
      margin-right: 10px;
    }

    #graph {
      width: 100%;
      height: 500px;
    }

    input {
      width: 60px;
    }

    .nav-bar {
  position: fixed;
  bottom: 0;
  left: 0;
  width: 100%;
  background-color: #ffffff00;
  color: white;
  display: flex;
  justify-content: space-between;
  align-items: center;
  padding: 10px 0px;
  z-index: 1000;
  /* Removed box-shadow (edge gradient) */
}

.nav-bar button {
  background-color: rgba(255, 255, 255, 0);
  color: #007acc;
  border: none;
  padding: 8px 16px;
  font-size: 16px;
  font-weight: bold;
  cursor: pointer;
  border-radius: 4px;
  transition: background-color 0.2s ease;
  margin-right: 10px;
}

.nav-bar button:last-child {
  margin-right: 0;
}

.nav-bar button:hover {
  background-color: #f0f0f0;
}

.nav-left {
  display: flex;
  align-items: center;
}

code {
  border-left: 4px solid #11cc00; /* Blue line */
  padding-left: 12px;             /* Space between border and code */
  display: block;                 /* Ensures it spans full width */
  background-color: #f9f9f9;      /* Optional: code background */
}
  </style>

  <!-- Highlight.js stylesheet (theme: GitHub) -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github.min.css">

<!-- Highlight.js library -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>

<!-- Initialize highlighting -->
<script>hljs.highlightAll();</script>

</head>
<body>

  <h1>Chapter 2.2: Softmax Regression</h1>

  <p>We now extend our logisitc regression model to support multiple classes. In this model we can have as many classes
    as we want (we will denote the number of classes with K) and we model the probability that a given data point belongs to class i as follows:
  </p>

  <div class="math">\(\hat{y}_{i}=P(y=i|x)=\frac{e^{z_{i}}}{\sum_{k=1}^{K}e^{z_{k}}}\), where \(i\in[1,K]\), \(Z=WX+B\) and \(z_k=w_{k}^{T}x_{k}+b_{k}\)</div>
  
  <p>This is the softmax function and we'll see it a lot when we work on
    neural networks as it is generally used as the final layer of networks in classification problems (note that W is a weight matrix and B is a bias vector). We also take the \(\mathrm{argmax}\) (class with highest probability) as
    the classification that the model makes.
  </p>

  <p>For this problem we will use the cross entropy loss function. We will use this as we are modelling the probability
    distributions of classes conditioned on a given data point. Similarly to BCE, cross entropy measures the average number of bits needed to identify
    an event drawn from the estimated distribution rather than the true distribution. This is again related to
    entropy from information theory. Cross entropy is defined as:
  </p>

  <div class="math">\(\mathcal{L}(Z)=-\sum_{i=1}^{K}y_{i}\log(\hat{y}_{i})\)</div>

  Now we can do the usual and take the gradients to get:

  <div class="math">\(\begin{aligned}\nabla_{Z}\mathcal{L}(Z)&=\hat{Y}-Y\\
        \nabla_{W}\mathcal{L}(Z)&=(\hat{Y}-Y)X^{T}\\
        \nabla_{B}\mathcal{L}(Z)&=\hat{Y}-Y\end{aligned}\)</div>

  A derivation of the gradient with respect to \(Z\) can be found <a href="https://www.geeksforgeeks.org/machine-learning/derivative-of-the-softmax-function-and-the-categorical-cross-entropy-loss/">here</a>. 
  Recall that \(Z=WX+B\), so the gradients with respect to W and B can be found using chain rule:

  <div class="math">\(\begin{align}\frac{\partial\mathcal{L}}{\partial W}&=\frac{\partial\mathcal{L}}{\partial Z}\frac{\partial Z}{\partial W}=(\hat{Y}-Y)X^{T}\\
                      \frac{\partial\mathcal{L}}{\partial B}&=\frac{\partial\mathcal{L}}{\partial Z}\frac{\partial Z}{\partial B}=\hat{Y}-Y\end{align}\)</div>

  Now that the gradients are known we have to find the minimum. There is no closed form solution to this problem when we set the gradients to zero so we can again use the gradient descent algorithm:

  <div class="math">\(\begin{align}W^{(t+1)}&=W^{(t)}-\eta\nabla_{W}\mathcal{L}(Z^{(t)})\\
    B^{(t+1)}&=B^{(t)}-\eta\nabla_{B}\mathcal{L}(Z^{(t)})\end{align}\)</div>



  <p>We can now implement it as is:</p>

  <pre><code class="language-python">
    import numpy as np

    def softmax(z):
        z -= np.max(z, axis=1, keepdims=True)  # for numerical stability
        exp_z = np.exp(z)
        return exp_z / np.sum(exp_z, axis=1, keepdims=True)

    def cross_entropy_loss(y_true, y_pred):
        # y_true and y_pred shape: (m, K)
        m = y_true.shape[0]
        log_likelihood = -np.log(y_pred[range(m), np.argmax(y_true, axis=1)] + 1e-15)
        return np.mean(log_likelihood)

    def one_hot(y, num_classes):
        return np.eye(num_classes)[y]

    def softmax_regression(X, y, num_classes, lr=0.1, epochs=1000):
        m, n = X.shape
        y_encoded = one_hot(y, num_classes)

        # Initialize weights and bias
        W = np.zeros((num_classes, n))
        b = np.zeros((num_classes, 1))

        for epoch in range(epochs):
            # Forward pass
            z = X @ W.T + b.T      # shape: (m, K)
            y_pred = softmax(z)    # shape: (m, K)

            # Loss
            if epoch % 100 == 0:
                loss = cross_entropy_loss(y_encoded, y_pred)
                print(f"Epoch {epoch}: Loss = {loss:.4f}")

            # Gradients
            dz = y_pred - y_encoded                # (m, K)
            dW = (dz.T @ X) / m                    # (K, n)
            db = np.mean(dz, axis=0, keepdims=True).T  # (K, 1)

            # Update
            W -= lr * dW
            b -= lr * db

        return W, b

    def predict(X, W, b):
        logits = X @ W.T + b.T
        probs = softmax(logits)
        return np.argmax(probs, axis=1)
  </code></pre>

  <a href="https://colab.research.google.com/drive/1n6WSLfD_ZeSDvzY6AS8I2yFaNyDNt4iD?usp=sharing" target="_blank">
  <button>Open in Google Colab</button>
  </a>

  <p>
    In the colab notebook we then train our model on the iris dataset to predict the flower species from the features of length and width of the sepals and petals. 
    The accuracy on the test dataset was 100% when a train-test split of 70-30 was used.
</p>

  <p>One really interesting real use case of this model was by <a href="https://www.horse4course-racetips.com/bill-benter.html">Bill Benter</a> who used it to predict the outcome of horse races.
These predictions allowed him to make lots of money from gambling on the horse races!</p>




<!-- Bottom Navigation Bar -->
<footer style="margin-top: 80px; padding: 30px 20px; text-align: center; font-size: 14px; color: #555;">
  <div style="margin-bottom: 10px;">
    <a href="../index.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Home</a> |
    <a href="logisticregression.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Previous</a> |
    <a href="#" style="margin: 0 10px; color: #007acc; text-decoration: none;">Next</a> |
    <a href="../tutorialhub.html" style="margin: 0 10px; color: #007acc; text-decoration: none;">Tutorial Hub</a>
  </div>
  <div>Â© 2025 Dominic Scocchera. All rights reserved.</div>
</footer>

</body>
</html>
